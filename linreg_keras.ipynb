{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "linreg_keras.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP3bzKhqNgqLvT/L9dTLuyQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/olegkleiman/tf2/blob/master/linreg_keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0Srqa4veUw5"
      },
      "source": [
        "This excerpt demonstrates the linear regression implementation with help of simplest Keras model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4SQefyOeVTg"
      },
      "source": [
        "# This excerpt demonstrates the linear regression implementation\n",
        "# with help of simplest Keras model\n",
        "from datetime import datetime\n",
        "from time import time\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "from plot_helpers import plot_to_image\n",
        "\n",
        "print(\"Num GPU Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
        "print(tf.test.gpu_device_name())\n",
        "\n",
        "slope = 0.4\n",
        "bias = 1.5\n",
        "data_size = 1000\n",
        "# 80% of the data is for training.\n",
        "train_pct = 0.8\n",
        "\n",
        "x_train = tf.random.uniform(shape=(data_size,))\n",
        "perturb = tf.random.normal(shape=(len(x_train),), stddev=0.1)\n",
        "y_train = slope * x_train + bias + perturb\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "data_train = train_dataset.take(800)\n",
        "data_test = train_dataset.skip(800)\n",
        "\n",
        "data_size = 800\n",
        "train_size = int(data_size * train_pct)\n",
        "x_test, y_test = x_train[train_size:], y_train[train_size:]\n",
        "\n",
        "\n",
        "# learning_rate = 0.05\n",
        "\n",
        "\n",
        "def lr_schedule(epoch, lr):\n",
        "    if epoch < 10:\n",
        "        lr = 0.05\n",
        "    else:\n",
        "        lr = lr * tf.math.exp(-0.1)\n",
        "\n",
        "    tf.summary.scalar('learning rate', data=lr, step=epoch)\n",
        "    return lr\n",
        "\n",
        "\n",
        "logdir = \"logs/keras/linreg/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "file_writer = tf.summary.create_file_writer(logdir + \"/metrics\")\n",
        "\n",
        "# Add image to TensorBoard\n",
        "with file_writer.as_default():\n",
        "    figure = plt.figure(figsize=(10, 10))\n",
        "    plt.scatter(x_train, y_train, color='blue',  label='y=0.4*x+1.5')\n",
        "    image = plot_to_image(figure)\n",
        "    tf.summary.image(\"plot\", image, step=0)\n",
        "\n",
        "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
        "# At the beginning of every epoch, this callback gets the updated learning rate value from lr_schedule\n",
        "lr_callback = keras.callbacks.LearningRateScheduler(lr_schedule, verbose=1)\n",
        "# This callback will stop the learning process if there is no improvement in minimizing loss for 3 epochs\n",
        "early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3, verbose=True)\n",
        "\n",
        "# output = Dense(1, activation='relu')(input_vals)\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Dense(units=1)\n",
        "])\n",
        "model.compile(loss=keras.losses.mean_squared_error,\n",
        "              optimizer=keras.optimizers.SGD(),  # lr=learning_rate),\n",
        "              # optimizer=tf.keras.optimizers.Adam(0.1)\n",
        "              metrics=[tf.metrics.mean_squared_error,\n",
        "                       tf.metrics.mean_absolute_error,\n",
        "                       tf.metrics.mean_absolute_percentage_error]\n",
        "              )\n",
        "\n",
        "# A very rough measurement of total learning time.\n",
        "# More precise measurement is use of custom callbacks\n",
        "start = time()\n",
        "history = model.fit(x_train, y_train,\n",
        "                    epochs=100,\n",
        "                    verbose=True,  # Suppress chatty output; use Tensorboard instead\n",
        "                    callbacks=[tensorboard_callback, lr_callback, early_stopping_callback])\n",
        "print('Total learning time: {} sec.'.format(time()-start))\n",
        "\n",
        "print(\"Average test loss: \", np.average(history.history['loss']))\n",
        "\n",
        "# Printing weights makes sense only for such simple model with only one layer\n",
        "weights = model.get_weights()\n",
        "print(weights)\n",
        "\n",
        "model.summary()\n",
        "print(model.predict([60, 25, 2]))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}